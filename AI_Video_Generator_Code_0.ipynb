{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary dependencies\n",
        "!pip install gradio moviepy transformers torch torchvision torchaudio -q\n",
        "!git clone https://github.com/lyogavin/train_your_own_sora.git sora_model\n",
        "!pip install git+https://github.com/openai/CLIP.git -q\n",
        "\n",
        "# Change to the sora_model directory\n",
        "%cd sora_model\n",
        "!pip install -r requirements.txt\n",
        "%cd .."
      ],
      "metadata": {
        "id": "xW3epP0ktHlY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae34e8b6-b14f-4c4d-f303-37043e66d900"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'sora_model'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 109 (delta 41), reused 95 (delta 36), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (109/109), 355.83 KiB | 7.12 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content/sora_model\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.0.11)\n",
            "Collecting diffusers==0.24.0 (from diffusers[torch]==0.24.0->-r requirements.txt (line 2))\n",
            "  Downloading diffusers-0.24.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.1.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.17.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.8.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.46.2)\n",
            "Collecting av (from -r requirements.txt (line 7))\n",
            "  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.24.0)\n",
            "Collecting decord (from -r requirements.txt (line 9))\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.2.2)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.5.1)\n",
            "Collecting omegaconf (from -r requirements.txt (line 12))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (0.18.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (0.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (4.12.3)\n",
            "Collecting xformers (from -r requirements.txt (line 17))\n",
            "  Downloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (6.3.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (0.26.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (8.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 1)) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.68.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (3.1.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 6)) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 6)) (4.66.6)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 8)) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 8)) (3.4.2)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 8)) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 8)) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 8)) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 10)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 10)) (2024.2)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->-r requirements.txt (line 12))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 13)) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 13)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 13)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 13)) (4.3.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 13)) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 13)) (1.3.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 13)) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->-r requirements.txt (line 16)) (2.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->-r requirements.txt (line 18)) (0.2.13)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 13)) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.24.0->diffusers[torch]==0.24.0->-r requirements.txt (line 2)) (3.21.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 13)) (5.0.1)\n",
            "Downloading diffusers-0.24.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=1908e0ff28fbda3b974161efe2301a13f2251cc772ed28268f9fbadae74e8ffb\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf, decord, av, xformers, diffusers\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.31.0\n",
            "    Uninstalling diffusers-0.31.0:\n",
            "      Successfully uninstalled diffusers-0.31.0\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 av-13.1.0 decord-0.6.0 diffusers-0.24.0 omegaconf-2.3.0 xformers-0.0.28.post3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "7d3ba0ca227d4664b3ffd6130319cddb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "from moviepy.editor import VideoFileClip\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Load BLIP model for captioning\n",
        "print(\"Loading BLIP model for better captions...\")\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Helper function to generate captions using BLIP\n",
        "def generate_caption(video_path):\n",
        "    print(f\"Processing video: {video_path}\")\n",
        "    clip = VideoFileClip(video_path)\n",
        "    frames = []\n",
        "    for i, frame in enumerate(clip.iter_frames(fps=0.5)):\n",
        "        if i > 20:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    # Convert frames to PIL images and generate captions\n",
        "    captions = []\n",
        "    for frame in frames:\n",
        "        pil_image = Image.fromarray(frame)\n",
        "        inputs = processor(pil_image, return_tensors=\"pt\").to(device)\n",
        "        outputs = model.generate(**inputs)\n",
        "        caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Combine captions into a single summary\n",
        "    combined_caption = \" \".join(captions)\n",
        "    print(f\"Generated captions: {captions}\")\n",
        "    return combined_caption\n",
        "\n",
        "# Function to overlay text using OpenCV with better readability\n",
        "def overlay_text_on_video(input_video_path, caption, output_video_path):\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Define font properties\n",
        "    font = cv2.FONT_HERSHEY_COMPLEX  # Changed to a more readable font\n",
        "    font_scale = 0.5  # Increased font size for better readability\n",
        "    font_color = (255, 255, 255)  # White text\n",
        "    thickness = 2\n",
        "    line_height = 10\n",
        "    padding = 15  # Increased padding for spacing\n",
        "\n",
        "    def wrap_text(text, max_width):\n",
        "        words = text.split()\n",
        "        lines = []\n",
        "        current_line = \"\"\n",
        "        for word in words:\n",
        "            test_line = f\"{current_line} {word}\".strip()\n",
        "            text_size = cv2.getTextSize(test_line, font, font_scale, thickness)[0]\n",
        "            if text_size[0] > max_width:\n",
        "                lines.append(current_line)\n",
        "                current_line = word\n",
        "            else:\n",
        "                current_line = test_line\n",
        "        lines.append(current_line)\n",
        "        return lines\n",
        "\n",
        "    wrapped_text = wrap_text(caption, width - 40)  # Added some extra margin\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Overlay each line of text with improved spacing\n",
        "        y_position = height - 50 - (len(wrapped_text) - 1) * (line_height + padding)\n",
        "        for line in wrapped_text:\n",
        "            text_size = cv2.getTextSize(line, font, font_scale, thickness)[0]\n",
        "            text_width = text_size[0]\n",
        "            text_height = text_size[1]\n",
        "\n",
        "            # Position text properly and add some padding\n",
        "            cv2.putText(frame, line, (20, y_position), font, font_scale, font_color, thickness)\n",
        "\n",
        "            y_position += text_height + line_height\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "# Generate a simple video with captions embedded\n",
        "def sora_generate_video(caption, input_video_path):\n",
        "    try:\n",
        "        output_video_path = \"generated_video.mp4\"\n",
        "        overlay_text_on_video(input_video_path, caption, output_video_path)\n",
        "        return output_video_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error during video generation: {e}\")\n",
        "        return None\n",
        "\n",
        "# Set up Gradio interface\n",
        "def process_video(input_video):\n",
        "    input_video_path = input_video\n",
        "    print(f\"Uploaded video path: {input_video_path}\")\n",
        "\n",
        "    # Generate video caption\n",
        "    caption = generate_caption(input_video_path)\n",
        "    print(f\"Caption generated: {caption}\")\n",
        "\n",
        "    # Generate a video placeholder with captions\n",
        "    generated_video_path = sora_generate_video(caption, input_video_path)\n",
        "    return generated_video_path, caption\n",
        "\n",
        "# Ensure the uploaded_videos directory exists\n",
        "os.makedirs(\"uploaded_videos\", exist_ok=True)\n",
        "\n",
        "# Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=process_video,\n",
        "    inputs=gr.Video(label=\"Upload Input Video\"),\n",
        "    outputs=[\n",
        "        gr.Video(label=\"Generated Video\"),\n",
        "        gr.Textbox(label=\"Generated Captions\")\n",
        "    ],\n",
        "    title=\"Rare-Human Action Video Generator\",\n",
        "    description=\"Upload a video, generate captions for the actions, and create a rare-human action video.\"\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "eIa9OGL_1qWG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19e3cc1c-9d2b-4e37-f129-67fe018d9f5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BLIP model for better captions...\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b081a0d546b5a3add9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b081a0d546b5a3add9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded video path: /tmp/gradio/e95fdc5e0b03f7b03541db7dc2c42b956644291d58bb0787bd25c526c7fa7648/input_video_1.mp4\n",
            "Processing video: /tmp/gradio/e95fdc5e0b03f7b03541db7dc2c42b956644291d58bb0787bd25c526c7fa7648/input_video_1.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated captions: ['a bed in a room with blue curtains', 'a bedroom with a bed and a window', 'a man sitting on a bed in a room', 'a man is lying on a bed in a room', 'a man standing in front of a bed']\n",
            "Caption generated: a bed in a room with blue curtains a bedroom with a bed and a window a man sitting on a bed in a room a man is lying on a bed in a room a man standing in front of a bed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:337: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded video path: /tmp/gradio/75a67c7f53e95705c994437040c2024959c3457f0812e34daa745138d5f6471e/input_video_2.mp4\n",
            "Processing video: /tmp/gradio/75a67c7f53e95705c994437040c2024959c3457f0812e34daa745138d5f6471e/input_video_2.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated captions: ['a man is sitting in a chair and talking to a cat', 'a man sitting at a table with a laptop', 'a man in a black shirt is standing in a kitchen', 'a man is standing in front of a counter', 'a man is cooking in a kitchen with a light on']\n",
            "Caption generated: a man is sitting in a chair and talking to a cat a man sitting at a table with a laptop a man in a black shirt is standing in a kitchen a man is standing in front of a counter a man is cooking in a kitchen with a light on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:337: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded video path: /tmp/gradio/b35de978686479b92b316e49b01893d33af2756643f3ef984430224de14cdc0d/input_video_3.mp4\n",
            "Processing video: /tmp/gradio/b35de978686479b92b316e49b01893d33af2756643f3ef984430224de14cdc0d/input_video_3.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated captions: ['a refrigerator and a refrigerator freezer in a kitchen', 'a woman in a red jacket is standing in front of a refrigerator', 'a refrigerator with a door open and a light on', 'a woman standing in front of a refrigerator', 'a woman in a red shirt is standing in front of a refrigerator', 'a woman in a red sweatshirt is standing in front of a refrigerator']\n",
            "Caption generated: a refrigerator and a refrigerator freezer in a kitchen a woman in a red jacket is standing in front of a refrigerator a refrigerator with a door open and a light on a woman standing in front of a refrigerator a woman in a red shirt is standing in front of a refrigerator a woman in a red sweatshirt is standing in front of a refrigerator\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:337: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded video path: /tmp/gradio/0af648b5777c0aaa6b32667cc8b83c8391fe5344a672e71e9cefb5978591bfa1/input_video_4.mp4\n",
            "Processing video: /tmp/gradio/0af648b5777c0aaa6b32667cc8b83c8391fe5344a672e71e9cefb5978591bfa1/input_video_4.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated captions: ['a man in a blue shirt', 'a man and woman standing in a room', 'a man standing in front of a green wall', 'a man in a white hat and a green wall', 'a man in a blue shirt and a woman in a white hat', 'a man in a white hat and a man in a blue shirt']\n",
            "Caption generated: a man in a blue shirt a man and woman standing in a room a man standing in front of a green wall a man in a white hat and a green wall a man in a blue shirt and a woman in a white hat a man in a white hat and a man in a blue shirt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:337: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded video path: /tmp/gradio/6bdb9b43087725543ad675b5b8fe20c589346454476e67734b5c0cb221f56424/input_video_5.mp4\n",
            "Processing video: /tmp/gradio/6bdb9b43087725543ad675b5b8fe20c589346454476e67734b5c0cb221f56424/input_video_5.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated captions: ['a man in a green and black suit sitting on a bed', 'a woman in a green and black outfit sitting on a bed', 'a woman in a green and black outfit sitting on a bed', 'a man is sitting on a bed in a room', 'a man in a room with a bed and a television', 'a woman is sitting on a bed in a room']\n",
            "Caption generated: a man in a green and black suit sitting on a bed a woman in a green and black outfit sitting on a bed a woman in a green and black outfit sitting on a bed a man is sitting on a bed in a room a man in a room with a bed and a television a woman is sitting on a bed in a room\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:337: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded video path: /tmp/gradio/61e14451355bcbc79f887f02129da350b657716bdef99286f826a3d5f1b26971/input_video_6.mp4\n",
            "Processing video: /tmp/gradio/61e14451355bcbc79f887f02129da350b657716bdef99286f826a3d5f1b26971/input_video_6.mp4\n",
            "Generated captions: ['a man is sitting in the back seat of a car', 'a man is sitting in a car with a bottle', 'a man sitting in a car with a beer', 'a man is seen in the video, and is seen on the screen', 'a man is driving a car with a cell']\n",
            "Caption generated: a man is sitting in the back seat of a car a man is sitting in a car with a bottle a man sitting in a car with a beer a man is seen in the video, and is seen on the screen a man is driving a car with a cell\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:337: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded video path: /tmp/gradio/87ec9c67e69128503262bb0540ffd15c06b232e8d6956f4028e9d7f1c6c070ae/input_video_7.mp4\n",
            "Processing video: /tmp/gradio/87ec9c67e69128503262bb0540ffd15c06b232e8d6956f4028e9d7f1c6c070ae/input_video_7.mp4\n",
            "Generated captions: ['a man is walking up the stairs in a house', 'a man is standing on the stairs and looking at a dog', 'a woman is standing in a doorway with her dog', 'a video of a man in a room with a camera', 'a woman is standing on the stairs in a room']\n",
            "Caption generated: a man is walking up the stairs in a house a man is standing on the stairs and looking at a dog a woman is standing in a doorway with her dog a video of a man in a room with a camera a woman is standing on the stairs in a room\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:337: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded video path: /tmp/gradio/6f12ad9431152392bbbc27c1c36d6f2dea84c8661d93a998f7d06eb73dbd1bf5/input_video_8.mp4\n",
            "Processing video: /tmp/gradio/6f12ad9431152392bbbc27c1c36d6f2dea84c8661d93a998f7d06eb73dbd1bf5/input_video_8.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated captions: ['a woman is sitting on the floor in a kitchen', 'a man is playing a video game in a kitchen', 'a woman is cleaning a kitchen with a hose', 'a man is standing in a room with a door', 'a man sitting on a toilet in a bathroom']\n",
            "Caption generated: a woman is sitting on the floor in a kitchen a man is playing a video game in a kitchen a woman is cleaning a kitchen with a hose a man is standing in a room with a door a man sitting on a toilet in a bathroom\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:337: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded video path: /tmp/gradio/3dd21a498b9a567b4cc91b43aac5b68335a5d10e3885fd9b4bdb7194ca566694/input_video_9.mp4\n",
            "Processing video: /tmp/gradio/3dd21a498b9a567b4cc91b43aac5b68335a5d10e3885fd9b4bdb7194ca566694/input_video_9.mp4\n",
            "Generated captions: ['a man and woman sitting on a couch', 'a woman sitting on a couch', 'a woman sitting on a couch', 'a woman sitting on a couch', 'a woman sitting on a couch', 'a woman sitting on a couch']\n",
            "Caption generated: a man and woman sitting on a couch a woman sitting on a couch a woman sitting on a couch a woman sitting on a couch a woman sitting on a couch a woman sitting on a couch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:337: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded video path: /tmp/gradio/57182cc8ea69bcca133dfeb7454f9f70a0759b7f5f3c715a7ce913d6aedb44ee/input_video_10.mp4\n",
            "Processing video: /tmp/gradio/57182cc8ea69bcca133dfeb7454f9f70a0759b7f5f3c715a7ce913d6aedb44ee/input_video_10.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated captions: ['a man is seen in the dark with his dog', 'the video shows the man being taken from the back of a car', 'a man is seen in the image of a man in a car', 'a man is seen in the video, and the video is still showing him', 'the video shows the man being taken from the back of a car']\n",
            "Caption generated: a man is seen in the dark with his dog the video shows the man being taken from the back of a car a man is seen in the image of a man in a car a man is seen in the video, and the video is still showing him the video shows the man being taken from the back of a car\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:337: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b081a0d546b5a3add9.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kwdqcp3qRdqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}