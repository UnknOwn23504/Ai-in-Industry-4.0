**A Report On**

**(Rare-Human Action Video Generator)**

**BY**

Team 7

Harsha Vardhan Reddy Ailuri - SE21UARI043

Heshika Pokala - SE21UARI050

Vasundhara Kandadi - SE21UARI062

Bhanu Chand Pothina - SE21UARI024

**CS 4124 : AI in Industry 4.0**

**Rare-Human Action Video Generator Setup and User Guide**

**1. Introduction**

This document provides step-by-step instructions for setting up and using the Rare-Human Action Video Generator. It also explains what outputs to expect and provides links to demo videos for reference. The project uses AI models to generate captions for input videos and create AI-synthesized videos based on those captions, focusing on rare-human actions. The system uses Gradio to offer an intuitive user interface for easy interaction.

**2. System Requirements**

- **Hardware**: GPU with CUDA support is highly recommended to ensure smooth operation, especially for video captioning and generation tasks.
- **Software**: Python 3.7+ and PyTorch with GPU support for faster processing.
- **Storage**: Ensure adequate disk space for storing model weights, uploaded videos, and generated videos.

**3. Installation and Setup**

Follow these steps to set up the system:

1. **Clone the Repository**
   
   ```
   git clone <repository_url>
   cd <repository_folder>
   ```

2. **Install Dependencies**
   
   Install all required Python packages using the command below:
   
   ```
   pip install -r requirements.txt
   ```

   Make sure your PyTorch installation supports GPU for faster processing, especially for video generation.

3. **Download Model Weights**

   Model weights will be automatically downloaded when the script is run for the first time. Ensure you have a stable internet connection during this step.

   Pre-trained models used:
   - BLIP Model: Salesforce/blip-image-captioning-base
   - AnimateDiff Pipeline: SG161222/Realistic\_Vision\_V5.1\_noVAE
   - Motion Adapter: guoyww/animatediff-motion-adapter-v1-5-2

4. **Run the Application**
   
   Start the Gradio interface by running:

   ```
   jupyter notebook AI_Video_Generator.ipynb
   ```

   This will open the Gradio interface in your browser, allowing you to upload videos, generate captions, and visualize the generated synthetic videos.

**4. How to Use the Application**

1. **Upload Video**
   - Use the Gradio interface to upload an input video.

2. **Generate Caption**
   - Once uploaded, the system will process the video using the BLIP model to generate descriptive captions for the actions present in the video.

3. **Generate AI Video**
   - The generated caption is used to create an AI-synthesized video using AnimateDiff.

4. **View Outputs**
   - The interface will display:
     - The original video with captions overlaid.
     - A text box containing the generated captions.
     - The AI-generated synthetic video visualizing the captioned action.

**5. Outputs to Expect**

- **Generated Captions**: A text summary of actions in the input video, provided in a text box.
- **Original Video with Captions**: The input video with the generated captions overlaid using OpenCV for easy visualization.
- **AI-Synthesized Video**: A new video generated by AnimateDiff, visualizing the captioned actions in a creative and synthetic manner.

**6. Directory Structure**

- **AI\_Video\_Generator.ipynb**: Main script that runs the Gradio interface.
- **uploaded\_videos/**: Stores uploaded videos.
- **generated\_videos/**: Stores generated synthetic videos.

**7. Troubleshooting Tips**

- **Slow Processing**: Ensure you are running the system on a GPU-enabled machine for efficient performance.
- **Model Download Issues**: If model weights fail to download, verify your internet connection and available disk space.

**8. Demo Video**

To view a demo of how the Rare-Human Action Video Generator works, click on the following link: (https://www.youtube.com/playlist?list=PLJxKi3ezq8dZzaQ7MT6TKFj5CdHzAjJWy)

**9. Future Enhancements**

- **Improved Captioning Quality**: Experiment with other pre-trained models to enhance the quality of captions.
- **Additional User Controls**: Provide more controls for adjusting generation parameters, such as guidance scale and the number of inference steps.
- **Comparison Feature**: Implement a side-by-side comparison view for better visualization of input and generated videos.

